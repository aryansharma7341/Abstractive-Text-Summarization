# Test Summarization

Text summarization is a pivotal task in natural language processing (NLP), aiming to condense large volumes of text into shorter, coherent representations while preserving the essential information. It finds applications in various domains, including news summarization, document summarization, and summarizing user-generated content on social media platforms.

One of the most prominent models used for text summarization is the Pegasus model, which is a transformer-based model developed by Google Research. Pegasus is pre-trained on a massive corpus of text using unsupervised learning techniques, enabling it to capture semantic relationships and contextual information effectively. Leveraging transfer learning, fine-tuning Pegasus for specific summarization tasks allows it to adapt its knowledge and parameters to generate high-quality summaries tailored to the input text.

Transfer learning involves taking a model pre-trained on a large dataset and fine-tuning it on a smaller, domain-specific dataset to enhance its performance on a particular task. In the context of text summarization, fine-tuning Pegasus involves training it on a dataset containing pairs of original texts and corresponding summaries. During fine-tuning, the model adjusts its parameters to minimize a loss function that measures the disparity between the generated summaries and the ground truth summaries, thereby improving its summarization capabilities.

<h1>Home Page Screenshort:</h1>

![Screenshot (269)](https://github.com/SanskarAgrawal17/test_summarization/assets/104454636/f3972b4f-4316-4d4b-8de8-3c3f31b90029)

<h1>Input Page Screenshort:</h1>

![Screenshot (303)](https://github.com/SanskarAgrawal17/test_summarization/assets/104454636/dbfc2a02-b4be-4cc2-b63e-94106fe834f8)

<h1>Output Page Screenshort:</h1>

![Screenshot (304)](https://github.com/SanskarAgrawal17/test_summarization/assets/104454636/dfce212e-6295-4895-87c6-d1d9fd438b15)





