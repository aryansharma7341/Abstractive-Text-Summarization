# -*- coding: utf-8 -*-
"""Text_Summarization Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Sgp18bUxlsrG4ScxRRxL572JEJspkd02
"""

!nvidia-smi

!pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q

!pip install --upgrade accelerate
!pip uninstall -y transformers accelerate
!pip install transformers accelerate

from transformers import pipeline
from datasets import load_dataset
import matplotlib.pyplot as plt
import pandas as pd
import torch.nn as nn

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer,BartConfig

import torch

device = "cuda" if torch.cuda.is_available() else "cpu"

device

tokenizer = AutoTokenizer.from_pretrained('google/pegasus-cnn_dailymail')

import accelerate

# Create an instance of DataLoaderConfiguration with the desired parameters
dataloader_config = accelerate.DataLoaderConfiguration(
    dispatch_batches=None,
    split_batches=False,
    even_batches=True,
    use_seedable_sampler=True
)

# Pass the dataloader_config instance to the Accelerator constructor
accelerator = accelerate.Accelerator(dataloader_config=dataloader_config)

model_peg = AutoModelForSeq2SeqLM.from_pretrained('google/pegasus-cnn_dailymail').to(accelerator.device)

dataset = load_dataset('cnn_dailymail','3.0.0')

dataset

from datasets import DatasetDict
import random

# Assuming
original_train_data = dataset['train']


total_rows = len(original_train_data)


desired_size = 2000

indices_to_keep = random.sample(range(total_rows), desired_size)

dataset['train'] = original_train_data.select(indices_to_keep)

from datasets import DatasetDict
import random

# Assuming
original_train_data = dataset['test']


total_rows = len(original_train_data)


desired_size = 2000

indices_to_keep = random.sample(range(total_rows), desired_size)

dataset['test'] = original_train_data.select(indices_to_keep)

from datasets import DatasetDict
import random

# Assuming
original_train_data = dataset['validation']


total_rows = len(original_train_data)


desired_size = 2000

indices_to_keep = random.sample(range(total_rows), desired_size)

dataset['validation'] = original_train_data.select(indices_to_keep)

dataset

def convert_data(batch):
  input_encoding = tokenizer(batch['article'], max_length=512, truncation = True )

  with tokenizer.as_target_tokenizer():
    target_encoding = tokenizer(batch['highlights'], max_length=200, truncation = True )


  return {
      'input_ids' : input_encoding['input_ids'],
      'attention_mask' : input_encoding['attention_mask'],
      'labels' : target_encoding['input_ids']
  }

data_samsum_pt = dataset.map(convert_data, batched= True )

data_samsum_pt['test']

from transformers import DataCollatorForSeq2Seq, TrainingArguments, Trainer

seq_data_collater = DataCollatorForSeq2Seq(tokenizer, model =model_peg )

training_arg = TrainingArguments(
    output_dir = 'Pegasus_model',
    num_train_epochs=1,
    warmup_steps=500,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    weight_decay = 0.01 ,
    logging_steps=10,
    evaluation_strategy='steps',
    eval_steps = 500,
    save_steps = 1e6 ,
    gradient_accumulation_steps=16
)

trainer = Trainer(model=model_peg,
                  args=training_arg,
                  tokenizer=tokenizer,
                  data_collator=seq_data_collater,
                  train_dataset=data_samsum_pt['test'],
                  eval_dataset=data_samsum_pt['validation'])

trainer.train()

trainer.evaluate(data_samsum_pt['test'])

# Input text for summarization
input_text = "Your input text goes here. This could be a longer piece of text that you want to summarize."

# Tokenize input text
input_ids = tokenizer.encode(input_text, return_tensors='pt')

# Generate summary using the model
summary_ids = trainer.generate(input_ids, max_length=50, num_beams=4, early_stopping=True)

# Decode the generated summary
summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# Print the generated summary
print("Generated Summary:", summary_text)

from google.colab import drive
from transformers import PegasusForConditionalGeneration

# Mount Google Drive
drive.mount('/content/drive')

# Assuming your model is stored in a variable called model_peg
model_peg.save_pretrained('/content/drive/MyDrive/pretrained_data')

